{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:27.968671Z",
     "start_time": "2023-08-14T22:04:27.948040Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:27.969111Z",
     "start_time": "2023-08-14T22:04:27.951235Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data/'\n",
    "VARIABLE_FOLDER = '../variables/'\n",
    "FIGURE_FOLDER = '../reports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:28.065847Z",
     "start_time": "2023-08-14T22:04:27.954018Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(DATA_FOLDER + '/processed/posts_and_comments.csv')\n",
    "df = pd.read_csv(DATA_FOLDER + 'processed/assigned_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:28.068230Z",
     "start_time": "2023-08-14T22:04:28.066636Z"
    }
   },
   "outputs": [],
   "source": [
    "# timestamps = df.date.to_list()\n",
    "# classes = df['query'].to_list()\n",
    "texts_list = df.content.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "# post_word_freq_dicts[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:35.646569Z",
     "start_time": "2023-08-14T22:04:35.645226Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:39.242945Z",
     "start_time": "2023-08-14T22:04:35.647865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"text-classification\", model=\"j-hartmann/sentiment-roberta-large-english-3-classes\",\n",
    "                              return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-14T22:04:39.246906Z",
     "start_time": "2023-08-14T22:04:39.244868Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def find_string_sentiment(text, query):\n",
    "    len_text = len(text)\n",
    "    words = text.split()  # Split the text into words\n",
    "    print(len(words))\n",
    "\n",
    "    sents_tokens = []  # List to store sentiment scores for individual tokens\n",
    "\n",
    "    for word in words:\n",
    "        sentiment_i = sentiment_analysis(word)  # Analyze sentiment using the pipeline\n",
    "        sentiment_scores = sentiment_i[0]  # Get the sentiment scores for the token\n",
    "        sents_tokens.append(sentiment_scores)\n",
    "\n",
    "    # print(sents_tokens)\n",
    "\n",
    "    # Calculate sentiment encoding for tokens based on the highest score\n",
    "    # sentiment_encoding_tokens = []\n",
    "    # for sentiment_scores in sents_tokens:\n",
    "    #     max_score_label = max(sentiment_scores, key=lambda x: x['score'])['label']\n",
    "    #     if max_score_label == 'negative':\n",
    "    #         sentiment_encoding_tokens.append(-1)\n",
    "    #     elif max_score_label == 'neutral':\n",
    "    #         sentiment_encoding_tokens.append(0)\n",
    "    #     elif max_score_label == 'positive':\n",
    "    #         sentiment_encoding_tokens.append(1)\n",
    "    #\n",
    "    # print(len(sentiment_encoding_tokens))\n",
    "    # # Calculate average sentiment scores for tokens\n",
    "    # avg_sent_tokens = np.mean(sentiment_encoding_tokens)\n",
    "    # print(avg_sent_tokens)\n",
    "    #\n",
    "    # # Analyze sentiment for the entire phrase using the pipeline\n",
    "    # sentiment_phrase = sentiment_analysis(text)\n",
    "    # sentiment_phrase_scores = sentiment_phrase[0]  # Get the sentiment scores for the phrase\n",
    "    # max_score_label = max(sentiment_phrase_scores, key=lambda x: x['score'])['label']\n",
    "    # if max_score_label == 'negative':\n",
    "    #     avg_sent_phrase = -1\n",
    "    # elif max_score_label == 'neutral':\n",
    "    #     avg_sent_phrase = 0\n",
    "    # elif max_score_label == 'positive':\n",
    "    #     avg_sent_phrase = 1\n",
    "    #\n",
    "    # print(max_score_label)\n",
    "\n",
    "    return words, sents_tokens\n",
    "\n",
    "# def find_string_sentiment(text, query):\n",
    "#     len_text = len(text)\n",
    "#\n",
    "#     # index = text.lower().find(query)\n",
    "#     indices = [match.start() for match in re.finditer(query, text.lower())]\n",
    "#\n",
    "#     sents = np.empty((len(indices), 3))\n",
    "#\n",
    "#     for i, index in enumerate(indices):\n",
    "#\n",
    "#         if (index - 256) < 0:\n",
    "#             start_idx = 0\n",
    "#         else:\n",
    "#             start_idx = (index - 256)\n",
    "#\n",
    "#         if (index + 256) > len_text:\n",
    "#             end_idx = len_text\n",
    "#         else:\n",
    "#             end_idx = (index + 256)\n",
    "#\n",
    "#         sentiment_i = sentiment_analysis(text[start_idx:end_idx])\n",
    "#         # print(sentiment_i[0][0])\n",
    "#         sents[i, :] = sentiment_i[0][0]['score'], sentiment_i[0][1]['score'], sentiment_i[0][2]['score']\n",
    "#\n",
    "#     return np.mean(sents, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-15T00:42:11.061690Z",
     "start_time": "2023-08-14T22:05:29.556222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4957\n",
      "Processed query: semaglutide\n",
      "1400\n",
      "Processed query: ozempic\n",
      "2290\n",
      "Processed query: wegovy\n",
      "2458\n",
      "Processed query: saxenda\n",
      "1505\n",
      "Processed query: tirzepatide\n",
      "2138\n",
      "Processed query: mounjaro\n",
      "4957\n",
      "Processed query: GLP-1\n",
      "4957\n",
      "Processed query: liraglutide\n",
      "2290\n",
      "Processed query: victoza\n",
      "4957\n",
      "Processed query: GLP1\n",
      "4957\n",
      "Processed query: exenatide\n",
      "4957\n",
      "Processed query: lixisenatide\n",
      "1025\n",
      "Processed query: bydureon\n",
      "1635\n",
      "Processed query: trulicity\n",
      "254\n",
      "Processed query: orforglipron\n",
      "948\n",
      "Processed query: byetta\n",
      "2290\n",
      "Processed query: rybelsus\n",
      "585\n",
      "Processed query: dulaglutide\n",
      "166\n",
      "Processed query: GLP1RA\n",
      "323\n",
      "Processed query: bydureon bcise\n",
      "181\n",
      "Processed query: adlyxin\n",
      "294\n",
      "Processed query: retatrutide\n",
      "232\n",
      "Processed query: ly3437943\n",
      "1058\n",
      "Processed query: GLP-1RA\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "sentiments = pd.DataFrame()\n",
    "\n",
    "lexicon = {}  # Initialize the lexicon dictionary\n",
    "\n",
    "for query in df['query'].unique():\n",
    "    query_data = df[df['query'] == query]  # Subset DataFrame for the current query\n",
    "    largest_post = query_data.loc[query_data['content'].apply(len).idxmax()]  # Get the largest post\n",
    "    try:\n",
    "        text = largest_post['content']\n",
    "        words, sent_tokens = find_string_sentiment(text, query)  # Apply sentiment analysis to the largest post\n",
    "        sentiments[query] = {'words': words, 'sent_tokens': sent_tokens}\n",
    "        # print(f\"Processed query: {query}\")\n",
    "        for word, sentiment_scores in zip(words, sent_tokens):\n",
    "            # print(sentiment_scores)\n",
    "            max_score_label = max(sentiment_scores, key=lambda x: x['score'])['label']\n",
    "            if word not in lexicon:\n",
    "                if max_score_label == 'neutral':\n",
    "                    lexicon[word] = 0\n",
    "                elif max_score_label == 'negative':\n",
    "                    lexicon[word] = sentiment_scores[0]['score'] * -1\n",
    "                else:\n",
    "                    lexicon[word] = sentiment_scores[2]['score']  # Positive score remains unchanged\n",
    "                # print(lexicon[word])\n",
    "        # print(i, end=\"\\r\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "\n",
    "# for i, text in (enumerate(texts_list)):\n",
    "#     try:\n",
    "#         sentiments.loc[i, :] = find_string_sentiment(text, df.iloc[i]['query'])\n",
    "#         print(i, end=\"\\r\")\n",
    "#     except:\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lexicon"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-14T22:04:39.316880Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out entries with a score of 0 and sort by magnitude of score\n",
    "filtered_lexicon = {word: score for word, score in lexicon.items() if score != 0}\n",
    "sorted_lexicon = dict(sorted(filtered_lexicon.items(), key=lambda item: abs(item[1]), reverse=True))\n",
    "\n",
    "# Extract keys and values from the sorted lexicon dictionary\n",
    "words = list(sorted_lexicon.keys())\n",
    "scores = list(sorted_lexicon.values())\n",
    "\n",
    "# Create a bar chart centered around 0\n",
    "plt.barh(words, scores, color=['red' if score < 0 else 'green' for score in scores])\n",
    "\n",
    "# Add a vertical line at 0\n",
    "plt.axvline(x=0, color='black', linewidth=0.8)\n",
    "\n",
    "# Customize plot labels and title\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Words')\n",
    "plt.title('Sentiment Scores of Words (sorted by magnitude)')\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(FIGURE_FOLDER + 'figures/Sentiment Analysis.png', bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-14T22:04:39.318120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-14T22:04:39.319688Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiment_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "sentiments['net'] = np.argmax(np.array(sentiments.loc[:, ['negative', 'neutral', 'positive']]), axis=1)\n",
    "# sentiments['net'] = sentiments['net'].apply(lambda x : sentiment_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-14T22:04:39.321617Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiments.index = df.index\n",
    "# sentiments = df.join(sentiments)\n",
    "# sentiments = pd.read_csv('sentiments.csv')\n",
    "# sentiments.to_csv(DATA_FOLDER + 'processed/sentiments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-14T22:04:39.324510Z"
    }
   },
   "outputs": [],
   "source": [
    "sentiments['net_t'] = sentiments['net'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics = pd.read_csv(DATA_FOLDER + '/processed/assigned_topics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments['topic'] = df['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df.merge(sentiments, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments.to_csv(DATA_FOLDER + 'processed/sentiments.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
